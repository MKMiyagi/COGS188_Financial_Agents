{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Nicholas Gao: background, problem statement, data \n",
    "- Ryan Chen: Expectations, Timeline, Evaluation, proposed solution, name\n",
    "- Matthew Miyagishima: Project Description, Abstract, Ethics and Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured\n",
    "\n",
    "The goal of our project is to design a stock trading agent that interacts with historical stock data that learnings optimal trading strategies using Markov Decision Processes (MDP) and Reinforcement Learning (RL). We will use historical stock data from Yahoo Finance. The data will be accessed through the yfinance Python package. The dataset stores key features such as Opening Price, Highest Price, Lowest Price, Closing Price, Trading Volume, and Date which are measured daily. First we will prepare the data by cleaning missing values and normalizing key features to ensure consistency. Then the data we will train an agent to buy, sell, or hold decisions based on past market trends utilizing reinforcement learning algorithm such as Q-Learning and Monte-Carlo Simulations. The performance of the agent will be evaulated using ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The stock market is a highly dynamic environment influenced by various factors, making it challenging to develop reliable trading strategies. Traditional rule-based approaches are often too rigid and fail to adapt to changing market conditions. Recently, machine learning techniques, particularly **Reinforcement Learning (RL)**, have become a popular tool for financial applications due to their ability to learn optimal strategies through direct interaction with the environment.\n",
    "\n",
    "In this project, we aim to build a **stock market trading agent** that leverages **Markov Decision Processes (MDPs)** as the underlying framework and applies **Q-Learning** and **Monte Carlo methods** to learn an optimal trading policy. The agent will use historical stock price data to simulate trading decisions and learn when to buy, sell, or hold a stock to maximize long-term profitability. We will focus on backtesting the agent’s strategy on historical data to assess its performance in a simulated environment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The objective of this project is to design a stock market trading agent that interacts with historical stock data and learns to optimize its trading strategy using **Markov Decision Processes (MDPs)**. The problem can be modeled as an MDP with the following components:\n",
    "\n",
    "- **State Space:** The state represents market conditions, derived from technical indicators such as recent price movements, moving averages, and volatility measures.\n",
    "- **Action Space:** The agent can choose one of three actions at each time step:\n",
    "  - **Buy:** Purchase a fixed quantity of the stock.\n",
    "  - **Sell:** Sell the currently held stock.\n",
    "  - **Hold:** Take no action and maintain the current position.\n",
    "- **Reward Function:** The reward at each step is the change in the portfolio value after taking an action, incentivizing profitable trades while penalizing losses or excessive trading.\n",
    "\n",
    "We will train the agent using two reinforcement learning approaches:\n",
    "1. **Monte Carlo Methods** for episodic policy evaluation and learning from full episodes of simulated trading.\n",
    "2. **Q-Learning**, a model-free method, to improve the agent’s strategy by updating Q-values for each state-action pair through iterative exploration.\n",
    "\n",
    "Performance will be evaluated using key metrics, including cumulative return, Sharpe ratio, and maximum drawdown.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use historical stock price data from the following sources:\n",
    "\n",
    "1. **Yahoo Finance API** ([https://finance.yahoo.com](https://finance.yahoo.com))\n",
    "   - Provides daily and intraday stock data.\n",
    "   - Variables: `Date`, `Open`, `High`, `Low`, `Close`, `Adjusted Close`, `Volume`.\n",
    "\n",
    "2. **S&P 500 Historical Data**\n",
    "   - Used as a benchmark for evaluating the trading agent’s performance.\n",
    "\n",
    "### Example Variables (Feature Set):\n",
    "- Price data (`Open`, `High`, `Low`, `Close`)\n",
    "- **Technical Indicators**: Moving averages (5-day, 20-day, 50-day), Relative Strength Index (RSI), Bollinger Bands, Momentum, Volatility, and MACD (Moving Average Convergence Divergence).\n",
    "\n",
    "### Data Preprocessing:\n",
    "- Handle missing values and normalize the features to ensure model stability.\n",
    "- Generate state representations by calculating technical indicators.\n",
    "- Define the reward function as the percentage change in portfolio value after each action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution to the problem statement above will be agents trained on stock trading. Our agents will be trained to buy, hold, or sell stocks in its portfolio to maximize its returns. With two different reinforcement learning approaches, we will evaluate how each trained agent behave differently. Agents will be trained on data mentioned above (price data and technical indicators) to make optimal stock trading decisions. While we are not considering another model as a benchmark, we will benchmark our agents with historical averages of the S&P 500.\n",
    "\n",
    "**Monte Carlo Methods**\n",
    "\n",
    "The agent will simulate the entire trading period using historical data of stocks in the training set to calculate reward values for actions taken at different states, as well as generate an optimal policy to take advantage of bullish or bearish markets.\n",
    "\n",
    "**Q-Learning**\n",
    "\n",
    "Q-Learning is an algorithm that learns the optimal action at each state, and the model simply needs to follow the selected actions. We will implement this using a hashtable where keys are each trading day and the values are the actions to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The main evaluation metric that we will use will be how much the agent grows/shrinks their portfolio over the test period. We will do so by giving the agent a portfolio to start off with at the beginning of the test period and evaluate the portfolio's worth daily throughout testing to measure how well the agent is doing. We believe that this is a good evaluation metric as the main goal of the agent is to maximize gains through buying, holding, and selling stocks.\n",
    "\n",
    "A mathematical representation of this metric would be\n",
    "\n",
    "$G_T = V_T - V_0$\n",
    "\n",
    "Where\n",
    "- $G_T$ is the gain/loss on day T\n",
    "- $V_T$ is the value of the portfolio on day T\n",
    "- $V_0$ is the value of the portfolio i the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a stock trading agent raises several ethical and privacy concerns, particularly in fairness, transparency, and security. The first concern is Market Fairness because algorithmic trading has the possiblity to contirbute to market manipulation, flash crashes, and unfair trading advantages for those with more computational resources. Furthermore, high-frequency trading firms already exploit the small inefficiencies that cannot be done by human traders. The next concern is that training models on historical data must be done cautiously to avoid overfitting to past trends which could mislead users into making bad financial decisions. Ensuring a transparent and interpretable agent is crucial because black-box reinforcement learning models can make unpredictable trades. Lastly, the societal impact should be considered because automated trading can influence the price of stocks and exacerbate market volatility and systematic risks. Making sure that there are safeguards within the agent if it is deployed at scale is necessary to prevent market spikes and crashes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Respond in a timely manner (within the day unless message sent after business hours) to communications via text, calls, or emails.\n",
    "* Attend all scheduled team meetings unless absense if communicated and excused beforehand.\n",
    "* Be punctual in attending team meetings.\n",
    "* Split work evenly and deliver assigned tasks in a timely manner.\n",
    "* Communicate openly about any issues/concerns/questions/etc. with other team members.\n",
    "* Collaborate effectively via the GitHub repository, including descriptive commit messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/13  |  8 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/14  |  11 PM |  Edit, finalize, and submit proposal; | \n",
    "| 2/20  | 8 AM  | Project Proposal |Discuss how to build the agent and assign tasks for members to lead\n",
    "| 2/27  | 8 PM  | Have data cleaning completed | Brainstorm how to start agent training and start programming\n",
    "| 3/6  | 8 PM  | Finalize initial Agent code;  | Begin programming for optimization; Discuss/edit project code; Complete project |\n",
    "| 3/13  | 8 PM  | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
