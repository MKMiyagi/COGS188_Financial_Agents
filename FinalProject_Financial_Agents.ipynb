{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ryan Chen\n",
    "- Nicholas Gao\n",
    "- Matthew Miyagishima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "The goal of our project is to design a stock trading agent that interacts with historical stock data that learnings optimal trading strategies using Markov Decision Processes (MDP) and Reinforcement Learning (RL). We will use historical stock data from Yahoo Finance. The data will be accessed through the yfinance Python package. The dataset stores key features such as Opening Price, Highest Price, Lowest Price, Closing Price, Trading Volume, and Date which are measured daily. First we will prepare the data by cleaning missing values and normalizing key features to ensure consistency. Then the data we will train an agent to buy, sell, or hold decisions based on past market trends utilizing reinforcement learning algorithm such as Q-Learning and Monte-Carlo Simulations. The performance of the agent will be evaulated using ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The stock market is a highly dynamic environment influenced by various factors, making it challenging to develop reliable trading strategies. Traditional rule-based approaches are often too rigid and fail to adapt to changing market conditions. Recently, machine learning techniques, particularly **Reinforcement Learning (RL)**, have become a popular tool for financial applications due to their ability to learn optimal strategies through direct interaction with the environment.\n",
    "\n",
    "In this project, we aim to build a **stock market trading agent** that leverages **Markov Decision Processes (MDPs)** as the underlying framework and applies **Q-Learning** and **Monte Carlo methods** to learn an optimal trading policy. The agent will use historical stock price data to simulate trading decisions and learn when to buy, sell, or hold a stock to maximize long-term profitability. We will focus on backtesting the agent’s strategy on historical data to assess its performance in a simulated environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The objective of this project is to design a stock market trading agent that interacts with historical stock data and learns to optimize its trading strategy using **Markov Decision Processes (MDPs)**. The problem can be modeled as an MDP with the following components:\n",
    "\n",
    "- **State Space:** The state represents market conditions, derived from technical indicators such as recent price movements, moving averages, and volatility measures.\n",
    "- **Action Space:** The agent can choose one of three actions at each time step:\n",
    "  - **Buy:** Purchase a fixed quantity of the stock.\n",
    "  - **Sell:** Sell the currently held stock.\n",
    "  - **Hold:** Take no action and maintain the current position.\n",
    "- **Reward Function:** The reward at each step is the change in the portfolio value after taking an action, incentivizing profitable trades while penalizing losses or excessive trading.\n",
    "\n",
    "We will train the agent using two reinforcement learning approaches:\n",
    "1. **Monte Carlo Methods** for episodic policy evaluation and learning from full episodes of simulated trading.\n",
    "2. **Q-Learning**, a model-free method, to improve the agent’s strategy by updating Q-values for each state-action pair through iterative exploration.\n",
    "\n",
    "Performance will be evaluated using key metrics, including cumulative return, Sharpe ratio, and maximum drawdown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use historical stock price data from the following sources:\n",
    "\n",
    "1. **Yahoo Finance API** ([https://finance.yahoo.com](https://finance.yahoo.com))\n",
    "   - Provides daily and intraday stock data.\n",
    "   - Variables: `Date`, `Open`, `High`, `Low`, `Close`, `Adjusted Close`, `Volume`.\n",
    "\n",
    "2. **S&P 500 Historical Data**\n",
    "   - Used as a benchmark for evaluating the trading agent’s performance.\n",
    "\n",
    "### Example Variables (Feature Set):\n",
    "- Price data (`Open`, `High`, `Low`, `Close`)\n",
    "- **Technical Indicators**: Moving averages (5-day, 20-day, 50-day), Relative Strength Index (RSI), Bollinger Bands, Momentum, Volatility, and MACD (Moving Average Convergence Divergence).\n",
    "\n",
    "### Data Preprocessing:\n",
    "- Handle missing values and normalize the features to ensure model stability.\n",
    "- Generate state representations by calculating technical indicators.\n",
    "- Define the reward function as the percentage change in portfolio value after each action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution to the problem statement above will be agents trained on stock trading. Our agents will be trained to buy, hold, or sell stocks in its portfolio to maximize its returns. With two different reinforcement learning approaches, we will evaluate how each trained agent behave differently. Agents will be trained on data mentioned above (price data and technical indicators) to make optimal stock trading decisions. While we are not considering another model as a benchmark, we will benchmark our agents with historical averages of the S&P 500.\n",
    "\n",
    "**Monte Carlo Methods**\n",
    "\n",
    "The agent will simulate the entire trading period using historical data of stocks in the training set to calculate reward values for actions taken at different states, as well as generate an optimal policy to take advantage of bullish or bearish markets.\n",
    "\n",
    "**Q-Learning**\n",
    "\n",
    "Q-Learning is an algorithm that learns the optimal action at each state, and the model simply needs to follow the selected actions. We will implement this using a hashtable where keys are each trading day and the values are the actions to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The main evaluation metric that we will use will be how much the agent grows/shrinks their portfolio percentage-wise over the test period. We will do so by giving the agent a portfolio to start off with at the beginning of the test period and evaluate the portfolio's worth daily throughout testing to measure how well the agent is doing. We believe that this is a good evaluation metric as the main goal of the agent is to maximize gains through buying, holding, and selling stocks.\n",
    "\n",
    "A mathematical representation of this metric would be\n",
    "\n",
    "$G_T = \\frac{V_T - V_0}{V_0}$\n",
    "\n",
    "Where\n",
    "- $G_T$ is the gain/loss on day T\n",
    "- $V_T$ is the value of the portfolio on day T\n",
    "- $V_0$ is the value of the portfolio i the beginning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_env import StockTrainingEnv\n",
    "from monte_carlo import monte_carlo_train, monte_carlo_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Stock Trading Environment\n",
    "env = StockTrainingEnv(tickers=['AAPL', 'TSLA', 'META', 'NVDA', 'GME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Monte Carlo Agent\n",
    "monte_carlo_q_table = monte_carlo_train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Agent Performance\n",
    "monte_carlo_evaluation = monte_carlo_eval(env, monte_carlo_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_gradient import PolicyNetwork, PolicyGradientAgent, train_policy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define dimensions for the agent\n",
    "state_dim = env.observation_space.shape[1]\n",
    "num_tickers = len(env.tickers)\n",
    "possible_trades = env.possible_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create agent\n",
    "agent = PolicyGradientAgent(\n",
    "    state_dim=state_dim,\n",
    "    num_tickers=num_tickers,\n",
    "    possible_trades=possible_trades\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train agent\n",
    "train_policy_gradient(env, agent, episodes=1000, gamma=0.99, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_model import ActorCriticNetwork, A2CAgent, a2c_train, a2c_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent and train\n",
    "a2c_agent = A2CAgent(env)\n",
    "a2c_agent_train = a2c_train(env, a2c_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate A2C agent\n",
    "a2c_agent_eval = a2c_eval(env, a2c_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
