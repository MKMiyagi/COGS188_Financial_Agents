{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Different Reinforcement Learning Algorithms for Stock Trading to Optimize Profitability\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ryan Chen\n",
    "- Nicholas Gao\n",
    "- Matthew Miyagishima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Stock trading is full of unpredictability and volatility. Without substantial understanding and experience in trading stocks, it is difficult for a person to trade actively and profit off of stocks. However, mathematical models and intelligent software can do a much better job of learning patterns and capitalizing on them. We explore the performance of value-based and policy-based reinforcement learning algorithms on trading stock data by comparing profits earned by Monte Carlo Q-learning, Monte Carlo policy gradient, and Advantage Actor-Critic on AAPL, TSLA, META, NVDA, and GME stocks from 2014 to 2024."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The stock market is a highly dynamic environment influenced by various factors, making it challenging to develop reliable trading strategies. Traditional rule-based approaches are often too rigid and fail to adapt to changing market conditions. Recently, machine learning techniques, particularly **Reinforcement Learning (RL)**, have become a popular tool for financial applications due to their ability to learn optimal strategies through direct interaction with the environment.\n",
    "\n",
    "In this project, we aim to build a **stock market trading agent** that leverages **Markov Decision Processes (MDPs)** as the underlying framework and applies **Q-Learning** and **Monte Carlo methods** to learn an optimal trading policy. The agent will use historical stock price data to simulate trading decisions and learn when to buy, sell, or hold a stock to maximize long-term profitability. We will focus on backtesting the agent’s strategy on historical data to assess its performance in a simulated environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The objective of this project is to design a stock market trading agent that interacts with historical stock data and learns to optimize its trading strategy using **Markov Decision Processes (MDPs)**. The problem can be modeled as an MDP with the following components:\n",
    "\n",
    "- **State Space:** The state provides information on stock prices over the past `window_size` days.\n",
    "- **Action Space:** The agent can choose one of three actions at each time step:\n",
    "  - **Buy:** Purchase some quantity of the stock.\n",
    "  - **Sell:** Sell some quantity of held stocks.\n",
    "  - **Hold:** Take no action and maintain the current position.\n",
    "- **Reward Function:** The reward function rewards the agent for making profitable actions on each stock:\n",
    "  - **Buy:** +5 reward for a valid purchase (based on agent's balance), +2 if agent overestimates number of stocks it can purchase, -5 reward if agent does not have enough money to even purchase one stock\n",
    "  - **Sell:** +5 for a sell that increases the agent's total portfolio value, -2 if agent loses porfolio value after selling, and -5 for an invalid sell action\n",
    "  - **Hold:** No reward or penalty\n",
    "  - **Bonus:** Percentage change in portfolio value * 10\n",
    "\n",
    "We will train the agent using three reinforcement learning approaches:\n",
    "1. **Monte Carlo Q-Learning** a model-free method that learns the optimal action at each state.\n",
    "2. **Monte Carlo Policy Gradient (REINFORCE)** a policy-based on-policy method that simulates entire processes and updates the policy according to the final reward.\n",
    "3. **A2C** an improvement on REINFORCE that introduces an actor-critic architecture where the actor learns the policy while the critic estimates the value function.\n",
    "\n",
    "We will evaluate the performance of each agent by looking at the final portfoliio value at the end of the time period. At the same time, we will be comparing their performance against that of the S&P500 across the same period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use historical stock price data from the following sources:\n",
    "\n",
    "1. **Yahoo Finance API** ([https://finance.yahoo.com](https://finance.yahoo.com))\n",
    "   - Provides daily and intraday stock data.\n",
    "   - Variables: `Date`, `Close`, `Split`.\n",
    "\n",
    "2. **S&P 500 Historical Data**\n",
    "   - Used as a benchmark for evaluating the trading agent’s performance.\n",
    "\n",
    "### Data Preprocessing:\n",
    "The Yahoo Finance API is a very developed API and has full data over the time period we are considering for the 5 chosen stocks. Thus no data preprocessing is required before using it to train our agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We propose three different solutions to the stock trading problem. Each of the solution algorithms will train an agent to interact with the environment through buying, selling, and holding on to stocks. These agents will be trained on data on AAPL, TSLA, META, NVDA, and GME stocks from 2014 to 2024 to optimize their profits.\n",
    "\n",
    "**Monte Carlo Q-Learning**\n",
    "\n",
    "The Monte Carlo Q-Learning algorithm is a value-based on-policy algorithm. In each episode, an action is selected using an epsilon-greedy selection at each state. The entire 10-year stock trading experience is simulated while the state, action, and reward are tracked. At the end of an episode, the tracked states, actions, and rewards are used to update the Q-table at each of those states to favor certain actions over others. The process continues until the agent learns the task well.\n",
    "\n",
    "Below is the pseudo-code for this algorithm:\n",
    "```\n",
    "for each episode:\n",
    "    initialize the environment\n",
    "    observe the initial state s\n",
    "    episode = []\n",
    "\n",
    "    while episode is not done:\n",
    "        choose action a using epsilon-greedy policy from Q(s, *)\n",
    "        take action a, record reward r and next state s'\n",
    "        store (s, a, r) in episode\n",
    "        s = s'\n",
    "    \n",
    "    G = 0\n",
    "    for each step (s, a, r) in episode, in reverse order:\n",
    "        G = gamma * G + r\n",
    "        if (s, a) is the first occurence in the episode:\n",
    "            append G to returns(s, a)\n",
    "            Q(s, a) = average(returns(s, a))\n",
    "return Q\n",
    "```\n",
    "\n",
    "**Monte Carlo Policy Gradient**\n",
    "\n",
    "The Monte Carlo Policy Gradient, or REINFORCE, algorithm is a policy-based on-policy training algorithm. It is similar to MC Q-Learning in that the agent simulates entire episodes of the task based on its policy before updating. However, this algorithm utilizes a neural network to represent its policy, and action selection is over a distribution rather than an epsilon-greedy selection.\n",
    "\n",
    "Below is the pseudo-code for this algorithm:\n",
    "```\n",
    "for each episode:\n",
    "    initialize the environment\n",
    "    observe the initial state s\n",
    "    episode = []\n",
    "\n",
    "    while episode is not done:\n",
    "        get distribution from a forward pass\n",
    "        choose action a using distribution\n",
    "        take action a, record reward r and next state s'\n",
    "        store (s, a, r) in episode\n",
    "        s = s'\n",
    "    \n",
    "    G = 0\n",
    "    for each step (s, a, r) in episode, in reverse order:\n",
    "        G = gamma * G + r\n",
    "        theta = theta + alpha * G * gradient(log(policy))\n",
    "```\n",
    "\n",
    "**A2C**\n",
    "\n",
    "The Advantage Actor-Critic algorithm is a policy-based on-policy training algorithm. It combines both policy gradient and value function estimation where the actor learns the policy while the critic learns to estimate the value of states. This algorithm achieves a more stable learning compared to REINFORCE due to reduced variance in the gradient estimates since it uses advantage for gradient calculations.\n",
    "\n",
    "Below is the pseudo-code for this algorithm:\n",
    "```\n",
    "for each episode:\n",
    "    initialize the environment\n",
    "    observe the initial state s\n",
    "    episode = []\n",
    "\n",
    "    while not done:\n",
    "        get action distribution from actor\n",
    "        choose action a using distribution\n",
    "        take action a, record reward r and next state s'\n",
    "\n",
    "        estimate value v of current state\n",
    "        get value of next state v'\n",
    "        set td target = r + gamma * v'\n",
    "\n",
    "        compute advantage A = td target - v\n",
    "\n",
    "        calculate squared loss for critic and update the critic\n",
    "        update the actor: theta = theta + alpha * gradient(log(policy) * A)\n",
    "\n",
    "        s = s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The main evaluation metric that we will use will be how much the agent grows/shrinks their portfolio percentage-wise over the test period. We will do so by giving the agent a portfolio to start off with at the beginning of the test period and evaluate the portfolio's worth daily throughout testing to measure how well the agent is doing. We believe that this is a good evaluation metric as the main goal of the agent is to maximize gains through buying, holding, and selling stocks.\n",
    "\n",
    "A mathematical representation of this metric would be\n",
    "\n",
    "$G_T = \\frac{V_T - V_0}{V_0}$\n",
    "\n",
    "Where\n",
    "- $G_T$ is the gain/loss on day T\n",
    "- $V_T$ is the value of the portfolio on day T\n",
    "- $V_0$ is the value of the portfolio i the beginning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_env import StockTrainingEnv\n",
    "from monte_carlo import monte_carlo_train, monte_carlo_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Stock Trading Environment\n",
    "env = StockTrainingEnv(tickers=['AAPL', 'TSLA', 'META', 'NVDA', 'GME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Monte Carlo Agent\n",
    "monte_carlo_q_table = monte_carlo_train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Agent Performance\n",
    "monte_carlo_evaluation = monte_carlo_eval(env, monte_carlo_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_gradient import PolicyNetwork, PolicyGradientAgent, train_policy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define dimensions for the agent\n",
    "state_dim = env.observation_space.shape[1]\n",
    "num_tickers = len(env.tickers)\n",
    "possible_trades = env.possible_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create agent\n",
    "agent = PolicyGradientAgent(\n",
    "    state_dim=state_dim,\n",
    "    num_tickers=num_tickers,\n",
    "    possible_trades=possible_trades\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train agent\n",
    "train_policy_gradient(env, agent, episodes=1000, gamma=0.99, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_model import ActorCriticNetwork, A2CAgent, a2c_train, a2c_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent and train\n",
    "a2c_agent = A2CAgent(env)\n",
    "a2c_agent_train = a2c_train(env, a2c_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate A2C agent\n",
    "a2c_agent_eval = a2c_eval(env, a2c_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
