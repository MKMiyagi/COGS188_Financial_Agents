{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ryan Chen\n",
    "- Nicholas Gao\n",
    "- Matthew Miyagishima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "The goal of our project is to design a stock trading agent that interacts with historical stock data that learnings optimal trading strategies using Markov Decision Processes (MDP) and Reinforcement Learning (RL). We will use historical stock data from Yahoo Finance. The data will be accessed through the yfinance Python package. The dataset stores key features such as Opening Price, Highest Price, Lowest Price, Closing Price, Trading Volume, and Date which are measured daily. First we will prepare the data by cleaning missing values and normalizing key features to ensure consistency. Then the data we will train an agent to buy, sell, or hold decisions based on past market trends utilizing reinforcement learning algorithm such as Q-Learning and Monte-Carlo Simulations. The performance of the agent will be evaulated using ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The stock market is a highly dynamic environment influenced by various factors, making it challenging to develop reliable trading strategies. Traditional rule-based approaches are often too rigid and fail to adapt to changing market conditions. Recently, machine learning techniques, particularly **Reinforcement Learning (RL)**, have become a popular tool for financial applications due to their ability to learn optimal strategies through direct interaction with the environment.\n",
    "\n",
    "In this project, we aim to build a **stock market trading agent** that leverages **Markov Decision Processes (MDPs)** as the underlying framework and applies **Q-Learning** and **Monte Carlo methods** to learn an optimal trading policy. The agent will use historical stock price data to simulate trading decisions and learn when to buy, sell, or hold a stock to maximize long-term profitability. We will focus on backtesting the agent’s strategy on historical data to assess its performance in a simulated environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The objective of this project is to design a stock market trading agent that interacts with historical stock data and learns to optimize its trading strategy using **Markov Decision Processes (MDPs)**. The problem can be modeled as an MDP with the following components:\n",
    "\n",
    "- **State Space:** The state represents market conditions, derived from technical indicators such as recent price movements, moving averages, and volatility measures.\n",
    "- **Action Space:** The agent can choose one of three actions at each time step:\n",
    "  - **Buy:** Purchase a fixed quantity of the stock.\n",
    "  - **Sell:** Sell the currently held stock.\n",
    "  - **Hold:** Take no action and maintain the current position.\n",
    "- **Reward Function:** The reward at each step is the change in the portfolio value after taking an action, incentivizing profitable trades while penalizing losses or excessive trading.\n",
    "\n",
    "We will train the agent using two reinforcement learning approaches:\n",
    "1. **Monte Carlo Methods** for episodic policy evaluation and learning from full episodes of simulated trading.\n",
    "2. **Q-Learning**, a model-free method, to improve the agent’s strategy by updating Q-values for each state-action pair through iterative exploration.\n",
    "\n",
    "Performance will be evaluated using key metrics, including cumulative return, Sharpe ratio, and maximum drawdown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use historical stock price data from the following sources:\n",
    "\n",
    "1. **Yahoo Finance API** ([https://finance.yahoo.com](https://finance.yahoo.com))\n",
    "   - Provides daily and intraday stock data.\n",
    "   - Variables: `Date`, `Open`, `High`, `Low`, `Close`, `Adjusted Close`, `Volume`.\n",
    "\n",
    "2. **S&P 500 Historical Data**\n",
    "   - Used as a benchmark for evaluating the trading agent’s performance.\n",
    "\n",
    "### Example Variables (Feature Set):\n",
    "- Price data (`Open`, `High`, `Low`, `Close`)\n",
    "- **Technical Indicators**: Moving averages (5-day, 20-day, 50-day), Relative Strength Index (RSI), Bollinger Bands, Momentum, Volatility, and MACD (Moving Average Convergence Divergence).\n",
    "\n",
    "### Data Preprocessing:\n",
    "- Handle missing values and normalize the features to ensure model stability.\n",
    "- Generate state representations by calculating technical indicators.\n",
    "- Define the reward function as the percentage change in portfolio value after each action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution to the problem statement above will be agents trained on stock trading. Our agents will be trained to buy, hold, or sell stocks in its portfolio to maximize its returns. With two different reinforcement learning approaches, we will evaluate how each trained agent behave differently. Agents will be trained on data mentioned above (price data and technical indicators) to make optimal stock trading decisions. While we are not considering another model as a benchmark, we will benchmark our agents with historical averages of the S&P 500.\n",
    "\n",
    "**Monte Carlo Methods**\n",
    "\n",
    "The agent will simulate the entire trading period using historical data of stocks in the training set to calculate reward values for actions taken at different states, as well as generate an optimal policy to take advantage of bullish or bearish markets.\n",
    "\n",
    "**Q-Learning**\n",
    "\n",
    "Q-Learning is an algorithm that learns the optimal action at each state, and the model simply needs to follow the selected actions. We will implement this using a hashtable where keys are each trading day and the values are the actions to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The main evaluation metric that we will use will be how much the agent grows/shrinks their portfolio percentage-wise over the test period. We will do so by giving the agent a portfolio to start off with at the beginning of the test period and evaluate the portfolio's worth daily throughout testing to measure how well the agent is doing. We believe that this is a good evaluation metric as the main goal of the agent is to maximize gains through buying, holding, and selling stocks.\n",
    "\n",
    "A mathematical representation of this metric would be\n",
    "\n",
    "$G_T = \\frac{V_T - V_0}{V_0}$\n",
    "\n",
    "Where\n",
    "- $G_T$ is the gain/loss on day T\n",
    "- $V_T$ is the value of the portfolio on day T\n",
    "- $V_0$ is the value of the portfolio i the beginning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
